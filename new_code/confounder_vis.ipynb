{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "import os\n",
    "import utils\n",
    "from tqdm import tqdm_notebook\n",
    "from sklearn.model_selection import train_test_split\n",
    "import multiprocessing\n",
    "import json\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 1])\n"
     ]
    }
   ],
   "source": [
    "import data, model, interpretation, utils, vis_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = np.load(\"test_data.npy\")\n",
    "test_label = np.load(\"test_label.npy\")\n",
    "\n",
    "test_data = np.reshape(test_data, (251, 1, 64, 64, 64))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize confounder pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = glm.perform_glm()\n",
    "confounder_mod = model.Confounder3DCNN(mask=mask, in_num_ch=1, img_size=(64,64,64), inter_num_ch=16, fc_num_ch=16,\n",
    "                                    conv_act='relu', fc_act='tanh').to(torch.device('cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brain_map = vis_utils.load_nifti('aal.nii.gz')\n",
    "brain_areas = np.unique(brain_map)[1:]  # omit background\n",
    "\n",
    "area_masks = []\n",
    "for area in tqdm_notebook(brain_areas):\n",
    "    area_mask = np.zeros_like(brain_map)\n",
    "    area_mask[brain_map == area] = 1\n",
    "    area_mask = vis_utils.resize_image(area_mask, (64, 64, 64), interpolation=0)\n",
    "    area_masks.append(area_mask)\n",
    "\n",
    "area_names = ['Precentral_L', 'Precentral_R', 'Frontal_Sup_L', 'Frontal_Sup_R', 'Frontal_Sup_Orb_L', 'Frontal_Sup_Orb_R', 'Frontal_Mid_L', 'Frontal_Mid_R', 'Frontal_Mid_Orb_L', 'Frontal_Mid_Orb_R', 'Frontal_Inf_Oper_L', 'Frontal_Inf_Oper_R', 'Frontal_Inf_Tri_L', 'Frontal_Inf_Tri_R', 'Frontal_Inf_Orb_L', 'Frontal_Inf_Orb_R', 'Rolandic_Oper_L', 'Rolandic_Oper_R', 'Supp_Motor_Area_L', 'Supp_Motor_Area_R', 'Olfactory_L', 'Olfactory_R', 'Frontal_Sup_Medial_L', 'Frontal_Sup_Medial_R', 'Frontal_Med_Orb_L', 'Frontal_Med_Orb_R', 'Rectus_L', 'Rectus_R', 'Insula_L', 'Insula_R', 'Cingulum_Ant_L', 'Cingulum_Ant_R', 'Cingulum_Mid_L', 'Cingulum_Mid_R', 'Cingulum_Post_L', 'Cingulum_Post_R', 'Hippocampus_L', 'Hippocampus_R', 'ParaHippocampal_L', 'ParaHippocampal_R', 'Amygdala_L', 'Amygdala_R', 'Calcarine_L', 'Calcarine_R', 'Cuneus_L', 'Cuneus_R', 'Lingual_L', 'Lingual_R', 'Occipital_Sup_L', 'Occipital_Sup_R', 'Occipital_Mid_L', 'Occipital_Mid_R', 'Occipital_Inf_L', 'Occipital_Inf_R', 'Fusiform_L', 'Fusiform_R', 'Postcentral_L', 'Postcentral_R', 'Parietal_Sup_L', 'Parietal_Sup_R', 'Parietal_Inf_L', 'Parietal_Inf_R', 'SupraMarginal_L', 'SupraMarginal_R', 'Angular_L', 'Angular_R', 'Precuneus_L', 'Precuneus_R', 'Paracentral_Lobule_L', 'Paracentral_Lobule_R', 'Caudate_L', 'Caudate_R', 'Putamen_L', 'Putamen_R', 'Pallidum_L', 'Pallidum_R', 'Thalamus_L', 'Thalamus_R', 'Heschl_L', 'Heschl_R', 'Temporal_Sup_L', 'Temporal_Sup_R', 'Temporal_Pole_Sup_L', 'Temporal_Pole_Sup_R', 'Temporal_Mid_L', 'Temporal_Mid_R', 'Temporal_Pole_Mid_L', 'Temporal_Pole_Mid_R', 'Temporal_Inf_L', 'Temporal_Inf_R', 'Cerebelum_Crus1_L', 'Cerebelum_Crus1_R', 'Cerebelum_Crus2_L', 'Cerebelum_Crus2_R', 'Cerebelum_3_L', 'Cerebelum_3_R', 'Cerebelum_4_5_L', 'Cerebelum_4_5_R', 'Cerebelum_6_L', 'Cerebelum_6_R', 'Cerebelum_7b_L', 'Cerebelum_7b_R', 'Cerebelum_8_L', 'Cerebelum_8_R', 'Cerebelum_9_L', 'Cerebelum_9_R', 'Cerebelum_10_L', 'Cerebelum_10_R', 'Vermis_1_2', 'Vermis_3', 'Vermis_4_5', 'Vermis_6', 'Vermis_7', 'Vermis_8', 'Vermis_9', 'Vermis_10']\n",
    "\n",
    "# Merge left and right areas.\n",
    "merged_area_names = [name[:-2] for name in area_names[:108:2]] + area_names[108:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_relevance_per_area(relevance_map, normalize=True):\n",
    "    relevances = np.zeros(len(area_masks))\n",
    "    print(relevances.shape)\n",
    "    for i, area_mask in enumerate(area_masks):\n",
    "        relevances[i] = np.sum(relevance_map * area_mask)\n",
    "    if normalize:\n",
    "        relevances /= relevances.sum()  # make all areas sum to 1\n",
    "\n",
    "    # Merge left and right areas.\n",
    "    merged_relevances = np.concatenate([relevances[:108].reshape(-1, 2).sum(1), relevances[108:]])\n",
    "\n",
    "    return sorted(zip(merged_area_names, merged_relevances), key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run loop over dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shape: (1, 64, 64, 64)\n",
    "# relevance maps; to be averaged after summing\n",
    "map_AD_correct = np.zeros((1, 64, 64, 64))\n",
    "map_AD_wrong = np.zeros((1, 64, 64, 64))\n",
    "map_NC_correct = np.zeros((1, 64, 64, 64))\n",
    "map_NC_wrong = np.zeros((1, 64, 64, 64))\n",
    "map_AD = np.zeros((1, 64, 64, 64))\n",
    "map_NC = np.zeros((1, 64, 64, 64))\n",
    "\n",
    "# counts... to keep track and average later\n",
    "count_AD_correct = 0\n",
    "count_AD_wrong = 0\n",
    "count_NC_correct = 0\n",
    "count_NC_wrong = 0\n",
    "count_AD = 0\n",
    "count_NC = 0\n",
    "\n",
    "# average of test images\n",
    "img_AD_correct = np.zeros((64, 64, 64))\n",
    "img_AD_wrong = np.zeros((64, 64, 64))\n",
    "img_NC_correct = np.zeros((64, 64, 64))\n",
    "img_NC_wrong = np.zeros((64, 64, 64))\n",
    "img_AD = np.zeros((64, 64, 64))\n",
    "img_NC = np.zeros((64, 64, 64))\n",
    "img_global = np.zeros((64, 64, 64)) # average of entire dataset\n",
    "\n",
    "# dict of top brain areas; to be summed\n",
    "dict_AD_correct = {}\n",
    "dict_AD_wrong = {}\n",
    "dict_NC_correct = {}\n",
    "dict_NC_wrong = {}\n",
    "dict_AD = {}\n",
    "dict_NC = {}\n",
    "\n",
    "import interpretation\n",
    "import importlib\n",
    "importlib.reload(interpretation)\n",
    "\n",
    "for i in range(251):\n",
    "  #if i % 20 == 0:\n",
    "  print(\"i: \", i)\n",
    "  label = test_label[i]\n",
    "  relevance_map_backprop, pred_label, _ = interpretation.sensitivity_analysis(net, test_data[i], cuda=False, verbose=True)\n",
    "  case = -1\n",
    "  if label == 1 and pred_label == 1:\n",
    "    case = 1\n",
    "    count_AD_correct += 1\n",
    "    img_AD_correct += test_data[i][0]\n",
    "    map_AD_correct += relevance_map_backprop\n",
    "  elif label == 1 and pred_label == 0:\n",
    "    case = 2\n",
    "    count_AD_wrong += 1\n",
    "    img_AD_wrong += test_data[i][0]\n",
    "    map_AD_wrong += relevance_map_backprop\n",
    "  elif label == 0 and pred_label == 0:\n",
    "    case = 3\n",
    "    count_NC_correct += 1\n",
    "    img_NC_correct += test_data[i][0]\n",
    "    map_NC_correct += relevance_map_backprop\n",
    "  elif label == 0 and pred_label == 1:\n",
    "    case = 4\n",
    "    count_NC_wrong += 1\n",
    "    img_NC_wrong += test_data[i][0]\n",
    "    map_NC_wrong += relevance_map_backprop\n",
    "  else:\n",
    "    print(\"entered invalid case with (label, pred_label) = \", label, pred_label)\n",
    "  \n",
    "  areas_top_ten = get_relevance_per_area(relevance_map_backprop[0])[:10]\n",
    "  dict_example = None\n",
    "  if case == 1:\n",
    "    dict_example = dict_AD_correct\n",
    "  elif case == 2:\n",
    "    dict_example = dict_AD_wrong\n",
    "  elif case == 3:\n",
    "    dict_example = dict_NC_correct\n",
    "  elif case == 4:\n",
    "    dict_example = dict_NC_wrong\n",
    "  else: \n",
    "    print(\"invalid case \", case)\n",
    "  for (area, pct) in areas_top_ten:\n",
    "    if area in dict_example:\n",
    "      dict_example[area] += pct\n",
    "    else:\n",
    "      dict_example[area] = pct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some post-processing\n",
    "map_AD = map_AD_correct + map_AD_wrong\n",
    "map_NC = map_NC_correct + map_NC_wrong\n",
    "count_AD = count_AD_correct + count_AD_wrong\n",
    "count_NC = count_NC_correct + count_NC_wrong\n",
    "img_AD = img_AD_correct + img_AD_wrong \n",
    "img_NC = img_NC_correct + img_NC_wrong\n",
    "img_global = img_AD + img_NC\n",
    "\n",
    "dict_AD = data.merge_dicts(dict_AD_correct, dict_AD_wrong)\n",
    "dict_NC = data.merge_dicts(dict_NC_correct, dict_NC_wrong)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(count_AD_correct, count_AD_wrong, count_NC_correct, count_NC_wrong)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need to average the maps across the number of examples that were summed up\n",
    "map_AD_correct /= count_AD_correct\n",
    "map_AD_wrong /= count_AD_wrong\n",
    "map_NC_correct /= count_NC_correct\n",
    "map_NC_wrong /= count_NC_wrong\n",
    "map_AD /= count_AD\n",
    "map_NC /= count_NC\n",
    "\n",
    "# also average the images\n",
    "img_AD_correct /= count_AD_correct\n",
    "img_AD_wrong /= count_AD_wrong\n",
    "img_NC_correct /= count_NC_correct\n",
    "img_NC_wrong /= count_NC_wrong\n",
    "img_AD /= count_AD\n",
    "img_NC /= count_NC\n",
    "img_global /= 251"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we plot the averaged maps, for each category!\n",
    "# probably want to do some quantitative results with the vmin, vmax, overlay min/max\n",
    "vmin_AD_correct, vmax_AD_correct, overlay_vmin_AD_correct, overlay_vmax_AD_correct = vis_utils.plot_slices(img_global, num_slices = 7, overlay=map_AD_correct[0], \n",
    "                  overlay_vmax=np.percentile(map_AD_correct, 99.9), \n",
    "                  overlay_cmap=vis_utils.alpha_to_red_cmap)\n",
    "\n",
    "vmin_AD_wrong, vmax_AD_wrong, overlay_vmin_AD_wrong, overlay_vmax_AD_wrong = vis_utils.plot_slices(img_global, num_slices = 7, overlay=map_AD_wrong[0], \n",
    "                  overlay_vmax=np.percentile(map_AD_wrong, 99.9), \n",
    "                  overlay_cmap=vis_utils.alpha_to_red_cmap)\n",
    "\n",
    "vmin_NC_correct, vmax_NC_correct, overlay_vmin_NC_correct, overlay_vmax_NC_correct = vis_utils.plot_slices(img_global, num_slices = 7, overlay=map_NC_correct[0], \n",
    "                  overlay_vmax=np.percentile(map_NC_correct, 99.9), \n",
    "                  overlay_cmap=vis_utils.alpha_to_red_cmap)\n",
    "\n",
    "vmin_NC_wrong, vmax_NC_wrong, overlay_vmin_NC_wrong, overlay_vmax_NC_wrong = vis_utils.plot_slices(img_global, num_slices = 7, overlay=map_NC_wrong[0], \n",
    "                  overlay_vmax=np.percentile(map_NC_wrong, 99.9), \n",
    "                  overlay_cmap=vis_utils.alpha_to_red_cmap)\n",
    "\n",
    "vmin_AD, vmax_AD, overlay_vmin_AD, overlay_vmax_AD = vis_utils.plot_slices(img_global, num_slices = 7, overlay=map_AD[0], \n",
    "                  overlay_vmax=np.percentile(map_AD, 99.9), \n",
    "                  overlay_cmap=vis_utils.alpha_to_red_cmap)\n",
    "\n",
    "vmin_NC, vmax_NC, overlay_vmin_NC, overlay_vmax_NC = vis_utils.plot_slices(img_global, num_slices = 7, overlay=map_NC[0], \n",
    "                  overlay_vmax=np.percentile(map_NC, 99.9), \n",
    "                  overlay_cmap=vis_utils.alpha_to_red_cmap)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get relevance areas across each category\n",
    "from operator import itemgetter\n",
    "\n",
    "res_AD_correct = dict(sorted(dict_AD_correct.items(), key = itemgetter(1), reverse = True)[:5])\n",
    "res_AD_wrong = dict(sorted(dict_AD_wrong.items(), key = itemgetter(1), reverse = True)[:5])\n",
    "res_NC_correct = dict(sorted(dict_NC_correct.items(), key = itemgetter(1), reverse = True)[:5])\n",
    "res_NC_wrong = dict(sorted(dict_NC_wrong.items(), key = itemgetter(1), reverse = True)[:5])\n",
    "res_AD = dict(sorted(dict_AD.items(), key = itemgetter(1), reverse = True)[:5])\n",
    "res_NC = dict(sorted(dict_NC.items(), key = itemgetter(1), reverse = True)[:5])\n",
    "\n",
    "print(res_AD_correct)\n",
    "print(res_AD_wrong)\n",
    "print(res_NC_correct)\n",
    "print(res_NC_wrong)\n",
    "print(res_AD)\n",
    "print(res_NC)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
